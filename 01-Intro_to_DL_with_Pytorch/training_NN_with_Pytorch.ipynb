{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Binary Classification\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8,1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n",
    "\n",
    "####Another example\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(11,8),\n",
    "        nn.Linear(8,5),\n",
    "        nn.Linear(5,3),\n",
    "        nn.Linear(3,1)\n",
    "\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1152, 0.5580, 0.1986, 0.1282]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "####Multiclass classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Loss function to assess how good the model is. Goal is to minimize loss funtn. \n",
    "\n",
    "We use one hot encoding to transform the ground truth into a form that can be used to compare with the  predicted class\n",
    "\n",
    "torch.nn.functional as F\n",
    "\n",
    "CrossEntropy loss for classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating one-hot encoded labels\n",
    "\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Calculating cross entrpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weights and Biases \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Accessing the model parameters\n",
    "\n",
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updating weights\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr* grads0\n",
    "weight1 =weight1 - lr* grads1\n",
    "weight2 = weight2 - lr* grads2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the PyTorch optimizer\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Data Science Salary Dataset\n",
    "\n",
    "## Use Mean Sqaured Error Loss as the Loss Function for Regression Tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using MSELoss\n",
    "\n",
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat-y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Before the training loop# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(nn.Linear(4, 2),                       \n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "show_results(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
